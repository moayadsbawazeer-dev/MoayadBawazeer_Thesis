{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "e6d68d21123d43048ef5021262383603",
      "6917662b7f6445aca1662ff3aa5c25bf",
      "3be386809f9a47958b3d5fea645afba7",
      "853c827327ed475990ce0017ac32dba1",
      "db9cc68aa6d242819c2ef1b42621f5c5",
      "61a0186a84204713a15bfefae7812b5a",
      "1055763824b84f5e874ca83751b94278",
      "878c98a924a547ff8f42512eb01f120f",
      "5d9e19ec991f49e0876298685b90bb03",
      "442b535f0985458fa429c80ff3c21d02",
      "bcaa3900aeba48fa950901da1fb410e5",
      "acc365a90d994c9492b465f9e41b97e9",
      "dd441df08873494f993b9fecf2d304f9",
      "36898dda5e3842e59588940bf76f9507",
      "88b4060c52e847d99bc5f5299db1fd48",
      "45dfa6ea2b56435490386c3972b8130a",
      "809c1f0f4ba246bd83398abe7289ea78",
      "d4520db35af641a3a01c0f2e983876eb",
      "fd041157289844e1a43443238b42bec5",
      "64e67bc14e6443b093356d7c2b360bff",
      "41a50851e40a40b3b993844789e847f1",
      "6bf21c22ffd64addae49dd48156bb011",
      "7b11256e7af94299b4c5575635e5fd13",
      "13fc4690ba344e6d8b8bd4b4dc5b5df2",
      "e5fa144a56a64d29a68cbf3993bd8ee4",
      "000bef65cc3d4880aad38ac9e86bf4c9",
      "ce0bd12235ef4943b1cca62f22dd63a5",
      "b21b80a6c5034e56b845573e42c58329",
      "1b1dfbb8551e4dde87d31de21611bc31",
      "595cbfdf4d5f464f95fed027bf33c375",
      "50c75d30dfbd40ceb50d1a487dd755be",
      "06780a5d022e4574b9239fc3edad0aca",
      "b2ce95be0f70486a848c7e84efa097aa",
      "e6d6554dcf394305b86c9660d6de81d1",
      "3b30ad7bbcd34314baef34ca58e19c1d",
      "133880044326464c94ee0901f28ee8ee",
      "a2af60c2d4254f55be00bdd463544644",
      "bfb97a504e0b4071bbf0e6a9b0ae65ec",
      "c8400837bcf3472ab4d4179f56739345",
      "aaf17458f3d54221ac1a7488be398937",
      "a819263f34474d419496690db7c5551b",
      "200af6a55ab4447a8f615ae7c44b56e3",
      "b7acedf053e04a73b6f6ec9321b0899f",
      "c2674c1db42b47d8bfd006ba0019576f",
      "580138e8827a4c0fb1d1c52988ff8c52",
      "0f50c51688dc40be976efd0d2f6cdbfe",
      "9e784aedbf1e40d5958aca3c54ba9966",
      "3e42da0babb54dc499eaf237699103d1",
      "d1b889a6c7164fb1b82f9f919e8df390",
      "0861fc4f5b30401897565e21e2e01567",
      "c745ed2172fd45f796eed665efb19103",
      "64cb1cee18044845bd3c897e570cd1ba",
      "bab207d063bf4e9c97955f566f96e858",
      "80101502f68b4397b787fce265899dcc",
      "387a01a1a7c1485f8234aaa7f15518a7",
      "3fef1eceab684d93b1d892c827a29835",
      "7bc78a312b714809a5a8489e988416b1",
      "9502659bf3834b1bb9d4fbea7ead8edb",
      "1ba01506f999473aa9bc33e9a86f56f9",
      "717e97f2d34b45b5b6f7c93e5c984c04",
      "c5b40267dc424cfea46f459fb83083eb",
      "2b51ee5f97b146f8a8d22b7b89294a59",
      "636f16bf3b8742e38a152f28c2fc1571",
      "ec6c287fd26f477fac3dccc8257a4fad",
      "468e40072ff8450eb77c00c6b7c704f4",
      "2f4df5ec592b4834858a82dc3b393347",
      "f1faab7c52a7428ba7e3fd85eb8053e4",
      "8046ba9601754b2e84ae6d4fb2e0c2b3",
      "1af6926988504d92a114a50979bbf26d",
      "2c6f87c059a648aca99a480e05477322",
      "9a5ab400806b461da0ffce71b6122829",
      "8b756512c4d94574b78a93082fc29fbf",
      "50e3e02eaa2f4b46b3f974109a41ff9c",
      "9f3cc0c770c746038fd32f83b2acf83b",
      "2bf16614b30f4c239660131fc892908a",
      "3fabdb89e7074e48b4cedf5c9e1d2ae3",
      "5f9c1a61993145578253ea233f9196f9"
     ]
    },
    "id": "8YWVv25Mdttq",
    "outputId": "9a035ae1-923a-49e4-f35e-97f6ab8cda9f"
   },
   "outputs": [],
   "source": [
    "# Install required packages\n",
    "!pip install transformers torch scikit-learn matplotlib nltk\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from transformers import GPT2Tokenizer, GPT2ForSequenceClassification\n",
    "from torch.utils.data import DataLoader, TensorDataset, random_split\n",
    "from torch.optim import AdamW\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "from google.colab import drive\n",
    "\n",
    "# Initialize NLTK resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')\n",
    "# Define constants\n",
    "MAX_LEN = 128\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 10  # GPT models are computationally expensive; you can increase if needed\n",
    "LEARNING_RATE = 5e-5\n",
    "CLIP_VALUE = 1.0\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Load dataset\n",
    "df = pd.read_csv(\"/content/drive/MyDrive/Colab Notebooks/Moayad/Symptom2Disease.csv\")\n",
    "df.drop(\"Unnamed: 0\", axis=1, inplace=True)\n",
    "\n",
    "# Preprocessing\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(sent):\n",
    "    sent = sent.translate(str.maketrans('', '', string.punctuation)).strip()\n",
    "    words = word_tokenize(sent)\n",
    "    words = [word.lower() for word in words if word not in stop_words]\n",
    "    return \" \".join(words)\n",
    "\n",
    "df[\"text\"] = df[\"text\"].apply(clean_text)\n",
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "df[\"label\"] = label_encoder.fit_transform(df[\"label\"])\n",
    "num_classes = len(label_encoder.classes_)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df[\"text\"], df[\"label\"], test_size=0.2, random_state=42, stratify=df[\"label\"]\n",
    ")\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "tokenizer.pad_token = tokenizer.eos_token  # GPT2 doesn't have a pad token by default\n",
    "\n",
    "def tokenize_data(sentences, labels, max_len=MAX_LEN):\n",
    "    inputs = tokenizer(list(sentences), max_length=max_len, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "    return inputs['input_ids'], inputs['attention_mask'], torch.tensor(labels)\n",
    "\n",
    "# Tokenize datasets\n",
    "train_inputs, train_masks, train_labels = tokenize_data(X_train.tolist(), y_train.tolist())\n",
    "test_inputs, test_masks, test_labels = tokenize_data(X_test.tolist(), y_test.tolist())\n",
    "\n",
    "# Create DataLoader\n",
    "train_dataset = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "test_dataset = TensorDataset(test_inputs, test_masks, test_labels)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE)\n",
    "\n",
    "# Compute class weights for imbalance handling\n",
    "class_weights = compute_class_weight('balanced', classes=np.unique(y_train), y=y_train)\n",
    "class_weights = torch.tensor(class_weights, dtype=torch.float).to('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load GPT-2 model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = GPT2ForSequenceClassification.from_pretrained(\n",
    "    'gpt2',\n",
    "    num_labels=num_classes,\n",
    ")\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.config.pad_token_id = tokenizer.pad_token_id  # Set pad token\n",
    "model.to(device)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
    "\n",
    "# Loss function with class weights\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "\n",
    "# Training function\n",
    "def train_model():\n",
    "    history = {'loss': [], 'val_loss': [], 'accuracy': [], 'val_accuracy': []}\n",
    "    for epoch in range(EPOCHS):\n",
    "        model.train()\n",
    "        total_loss, correct_preds = 0, 0\n",
    "\n",
    "        for batch in train_loader:\n",
    "            batch_inputs, batch_masks, batch_labels = [b.to(device) for b in batch]\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(batch_inputs, attention_mask=batch_masks, labels=batch_labels)\n",
    "            loss = outputs.loss\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), CLIP_VALUE)\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            correct_preds += (torch.argmax(outputs.logits, axis=1) == batch_labels).sum().item()\n",
    "\n",
    "        train_loss = total_loss / len(train_loader)\n",
    "        train_acc = correct_preds / len(train_dataset)\n",
    "\n",
    "        model.eval()\n",
    "        val_loss, val_correct_preds = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for batch in test_loader:\n",
    "                batch_inputs, batch_masks, batch_labels = [b.to(device) for b in batch]\n",
    "                outputs = model(batch_inputs, attention_mask=batch_masks, labels=batch_labels)\n",
    "                loss = outputs.loss\n",
    "                val_loss += loss.item()\n",
    "                val_correct_preds += (torch.argmax(outputs.logits, axis=1) == batch_labels).sum().item()\n",
    "\n",
    "        val_loss /= len(test_loader)\n",
    "        val_acc = val_correct_preds / len(test_dataset)\n",
    "\n",
    "        history['loss'].append(train_loss)\n",
    "        history['val_loss'].append(val_loss)\n",
    "        history['accuracy'].append(train_acc)\n",
    "        history['val_accuracy'].append(val_acc)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{EPOCHS}\")\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Accuracy: {train_acc:.4f}\")\n",
    "        print(f\"Val Loss: {val_loss:.4f}, Val Accuracy: {val_acc:.4f}\")\n",
    "\n",
    "    return history\n",
    "\n",
    "# Train model\n",
    "history = train_model()\n",
    "\n",
    "# Plot training history\n",
    "ax = plt.figure().gca()\n",
    "ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "ax.plot(history['loss'], label='Train Loss')\n",
    "ax.plot(history['val_loss'], label='Validation Loss')\n",
    "plt.title('Loss over Training Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "ax = plt.figure().gca()\n",
    "ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "ax.plot(history['accuracy'], label='Train Accuracy')\n",
    "ax.plot(history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Accuracy over Training Epochs')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Evaluate model\n",
    "model.eval()\n",
    "predictions, true_labels = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        batch_inputs, batch_masks, batch_labels = [b.to(device) for b in batch]\n",
    "        outputs = model(batch_inputs, attention_mask=batch_masks)\n",
    "        predictions.extend(torch.argmax(outputs.logits, axis=1).cpu().numpy())\n",
    "        true_labels.extend(batch_labels.cpu().numpy())\n",
    "\n",
    "# Classification report\n",
    "print(classification_report(true_labels, predictions, target_names=label_encoder.classes_, zero_division=0))\n",
    "print(f\"Accuracy: {accuracy_score(true_labels, predictions):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "krv-jotEdu5b",
    "outputId": "2cdc5d4d-39b1-40ac-a85c-8f39270f747f"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "y_true = np.array(true_labels)\n",
    "y_pred = np.array(predictions)\n",
    "\n",
    "# Confusion matrix\n",
    "conf_mat = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "# Class names from label encoder\n",
    "category_labels = label_encoder.classes_\n",
    "\n",
    "# Plot styled like your Naive Bayes + LDA confusion matrix\n",
    "plt.figure(figsize=(16, 14))\n",
    "sns.set(font_scale=1.0)\n",
    "\n",
    "sns.heatmap(\n",
    "    conf_mat,\n",
    "    annot=True,\n",
    "    fmt='d',\n",
    "    cmap='YlGnBu',                   # same colormap style\n",
    "    xticklabels=category_labels,\n",
    "    yticklabels=category_labels\n",
    ")\n",
    "\n",
    "plt.title('Confusion Matrix (GPT-2-based Classifier)')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Actual')\n",
    "\n",
    "plt.xticks(rotation=45, ha='right')  # same label style as your example\n",
    "plt.yticks(rotation=0)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
